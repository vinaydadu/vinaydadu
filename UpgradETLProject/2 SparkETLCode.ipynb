{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark code to extract data from hadoop into dataframes and export onto S3 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9ac1feda62408d99f35425ac441a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1643525333131_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-64-104.ec2.internal:20888/proxy/application_1643525333131_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-64-104.ec2.internal:8042/node/containerlogs/container_1643525333131_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Importing required libraries\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68769d0d637c45f7af7b95414cd02e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b434a086cc274a57991baf3817664a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##creating a custom schema based on incoming file. columnOrder also needs to be maintained\n",
    "columns = StructType([StructField('year', IntegerType(),True),\n",
    "                        StructField('month', StringType(),True),\n",
    "                        StructField('day', IntegerType(),True),\n",
    "                        StructField('weekday', StringType(),True),\n",
    "                        StructField('hour', IntegerType(),True),\n",
    "                        StructField('atm_status', StringType(),True),\n",
    "                        StructField('atm_id', StringType(),True),\n",
    "                        StructField('atm_manufacturer', StringType(),True),\n",
    "                        StructField('atm_location', StringType(),True),\n",
    "                        StructField('atm_streetname', StringType(),True),\n",
    "                        StructField('atm_street_number', IntegerType(),True),\n",
    "                        StructField('atm_zipcode', IntegerType(),True),\n",
    "                        StructField('atm_lat', DoubleType(),True),\n",
    "                        StructField('atm_lon', DoubleType(),True),\n",
    "                        StructField('currency', StringType(),True),\n",
    "                        StructField('card_type', StringType(),True),\n",
    "                        StructField('transaction_amount', IntegerType(),True),\n",
    "                        StructField('service', StringType(),True),\n",
    "                        StructField('message_code', StringType(),True),\n",
    "                        StructField('message_text', StringType(),True),\n",
    "                        StructField('weather_lat', DoubleType(),True),\n",
    "                        StructField('weather_lon', DoubleType(),True),\n",
    "                        StructField('weather_city_id', IntegerType(),True),\n",
    "                        StructField('weather_city_name', StringType(),True),\n",
    "                        StructField('temp', DoubleType(),True),\n",
    "                        StructField('pressure', IntegerType(),True),\n",
    "                        StructField('humidity', IntegerType(),True),\n",
    "                        StructField('wind_speed', IntegerType(),True),\n",
    "                        StructField('wind_deg', IntegerType(),True),\n",
    "                        StructField('rain_3h', DoubleType(),True),\n",
    "                        StructField('clouds_all', IntegerType(),True),\n",
    "                        StructField('weather_id', IntegerType(),True),\n",
    "                        StructField('weather_main', StringType(),True),\n",
    "                        StructField('weather_description', StringType(),True),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e390290df6411884fda87a75e59501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##read csv from user/livy folder with custom schema\n",
    "df=spark.read.csv(\"/user/livy/SRC_ATM_TRANS/part-m-00000\", header = True, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a37b38e0c554e6391d4ab2d245bbfe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "|year|  month|day|weekday|hour|atm_status|atm_id|atm_manufacturer|atm_location|     atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|currency| card_type|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|  temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main| weather_description|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|     DKK|MasterCard|              1764|Withdrawal|        null|        null|     57.048|      9.935|        2616235|   NÃƒÂ¸rresundby|280.64|    1020|      93|         9|     250|   0.59|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|     DKK|      VISA|              1891|Withdrawal|        null|        null|     57.048|      9.935|        2616235|   NÃƒÂ¸rresundby|280.64|    1020|      93|         9|     250|   0.59|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     3|             NCR|       Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|     DKK|      VISA|              4166|Withdrawal|        null|        null|     56.139|      9.158|        2619426|            Ikast|281.15|    1011|     100|         6|     240|    0.0|        75|       300|     Drizzle|light intensity d...|\n",
      "|2017|January|  1| Sunday|   0|    Active|     4|             NCR|  Svogerslev|       BrÃƒÂ¸nsager|                1|       4000| 55.634| 12.018|     DKK|MasterCard|              5153|Withdrawal|        null|        null|     55.642|      12.08|        2614481|         Roskilde|280.61|    1014|      87|         7|     260|    0.0|        88|       701|        Mist|                mist|\n",
      "|2017|January|  1| Sunday|   0|    Active|     5|             NCR|        Nibe|             Torvet|                1|       9240| 56.983|  9.639|     DKK|MasterCard|              3269|Withdrawal|        null|        null|     56.981|      9.639|        2616483|             Nibe|280.64|    1020|      93|         9|     250|   0.59|        92|       500|        Rain|          light rain|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "##show a few rows to validate the csv import\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45485e166f5411bb61aa175103c7cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2468571"
     ]
    }
   ],
   "source": [
    "### Validating Counts\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20551458a2b24c0e9ec158a2d2ebbcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12"
     ]
    }
   ],
   "source": [
    "DIM_CARD_TYPE_raw=df.select(\"card_type\").distinct()\n",
    "DIM_CARD_TYPE_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6a82d1e4c54a2d8c774d0fc91df1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8685"
     ]
    }
   ],
   "source": [
    "DIM_DATE_raw=df.select(\"year\",\"month\",\"day\",\"hour\",\"weekday\").distinct()\n",
    "DIM_DATE_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a31c7ca22643a7889c1cca81a262d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113"
     ]
    }
   ],
   "source": [
    "DIM_ATM_raw=df.select(\"atm_id\",\"atm_manufacturer\",\"atm_location\",\"atm_streetname\",\"atm_lat\",\"atm_lon\").distinct()\n",
    "DIM_ATM_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c43a2bc034e418bb82dcea6a4ab7a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109"
     ]
    }
   ],
   "source": [
    "DIM_LOCATION_raw=df.select(\"atm_location\",\"atm_streetname\",\"atm_street_number\",\"atm_zipcode\",\"atm_lat\",\"atm_lon\").distinct()\n",
    "DIM_LOCATION_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5a1207530f4306af72e109065b5d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2468571"
     ]
    }
   ],
   "source": [
    "FACT_ATM_TRANS_raw=df.select(\"year\",\"month\",\"day\",\"hour\",\"card_type\",\"atm_id\",\"atm_manufacturer\",\"atm_location\",\"atm_streetname\",\"atm_street_number\",\"atm_zipcode\",\"atm_lat\",\"atm_lon\",\"atm_status\",\"currency\",\"service\",\"transaction_amount\",\"message_code\",\"message_text\",\"rain_3h\",\"clouds_all\",\"weather_id\",\"weather_main\",\"weather_description\")\n",
    "FACT_ATM_TRANS_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c78adfe6624f569cd5d583daf393bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RENAME atm_id to atm_number. atm_id will be generated later.\n",
    "DIM_ATM_renamed = DIM_ATM_raw.withColumnRenamed(\"atm_id\",\"atm_number\")\n",
    "FACT_ATM_TRANS_raw=FACT_ATM_TRANS_raw.withColumnRenamed(\"atm_id\",\"atm_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3a3ee0397e4d01820b4b3153bb60db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### generate *_id's \n",
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "DIM_DATE = DIM_DATE_raw.withColumn('date_id', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "DIM_CARD_TYPE = DIM_CARD_TYPE_raw.withColumn('card_id', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "DIM_ATM = DIM_ATM_renamed.withColumn('atm_id', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "DIM_LOCATION = DIM_LOCATION_raw.withColumn('location_id', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS_raw.withColumn('trans_id', row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e228612aa08e4a299a2293afafe0d989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+----+-------+\n",
      "|year|   month|day|hour|date_id|\n",
      "+----+--------+---+----+-------+\n",
      "|2017| January| 23|   2|      1|\n",
      "|2017|February| 26|   8|      2|\n",
      "|2017|   March| 12|  13|      3|\n",
      "|2017|   March| 14|  21|      4|\n",
      "|2017|   March| 21|   7|      5|\n",
      "|2017| January| 28|  23|      6|\n",
      "|2017|February| 14|  10|      7|\n",
      "|2017| January|  1|   6|      8|\n",
      "|2017| January| 25|  13|      9|\n",
      "|2017|February| 17|  22|     10|\n",
      "|2017|February| 21|  23|     11|\n",
      "|2017|   March| 28|  23|     12|\n",
      "|2017|   March| 31|  18|     13|\n",
      "|2017|   March| 23|  16|     14|\n",
      "|2017|   March| 21|  21|     15|\n",
      "|2017|   March| 27|  22|     16|\n",
      "|2017|   March| 28|  10|     17|\n",
      "|2017|   March| 31|   0|     18|\n",
      "|2017|   March| 13|  23|     19|\n",
      "|2017|February| 26|  20|     20|\n",
      "|2017|   March|  4|  10|     21|\n",
      "|2017|   March|  7|   3|     22|\n",
      "|2017|February|  4|  23|     23|\n",
      "|2017|February|  9|   3|     24|\n",
      "|2017|February|  9|  10|     25|\n",
      "|2017|   March|  3|  16|     26|\n",
      "|2017|   March|  4|  11|     27|\n",
      "|2017|   March| 21|   4|     28|\n",
      "|2017| January| 22|  12|     29|\n",
      "|2017|February| 26|  14|     30|\n",
      "|2017|February| 21|   0|     31|\n",
      "|2017|   March|  3|  15|     32|\n",
      "|2017|February| 26|  12|     33|\n",
      "|2017|February|  2|  18|     34|\n",
      "|2017|February|  9|  19|     35|\n",
      "|2017|   April|  3|   1|     36|\n",
      "|2017| January|  4|   1|     37|\n",
      "|2017|February| 15|   4|     38|\n",
      "|2017|February| 23|  10|     39|\n",
      "|2017|February| 25|  19|     40|\n",
      "|2017| January|  4|  18|     41|\n",
      "|2017|February| 16|  11|     42|\n",
      "|2017|   March| 21|   6|     43|\n",
      "|2017|February| 10|  14|     44|\n",
      "|2017|February| 14|   8|     45|\n",
      "|2017|February| 15|  18|     46|\n",
      "|2017| January| 24|   2|     47|\n",
      "|2017|   March|  1|  11|     48|\n",
      "|2017|   March| 30|   7|     49|\n",
      "|2017| January|  1|  16|     50|\n",
      "|2017| January| 14|   4|     51|\n",
      "|2017| January| 12|  22|     52|\n",
      "|2017|February|  7|  21|     53|\n",
      "|2017|February| 11|   2|     54|\n",
      "|2017|February| 24|   8|     55|\n",
      "|2017|   March| 16|  20|     56|\n",
      "|2017| January| 30|   2|     57|\n",
      "|2017|February|  6|  21|     58|\n",
      "|2017|   March| 29|  15|     59|\n",
      "|2017| January|  8|  16|     60|\n",
      "|2017| January| 18|  13|     61|\n",
      "|2017|February| 25|  21|     62|\n",
      "|2017|   March| 21|  17|     63|\n",
      "|2017|   March| 11|  12|     64|\n",
      "|2017|   March| 24|   9|     65|\n",
      "|2017| January| 13|  22|     66|\n",
      "|2017|February| 15|   6|     67|\n",
      "|2017|   March| 22|  12|     68|\n",
      "|2017|   March| 26|  14|     69|\n",
      "|2017| January|  8|   6|     70|\n",
      "|2017| January| 19|  19|     71|\n",
      "|2017|   April|  3|   6|     72|\n",
      "|2017|   March| 31|   8|     73|\n",
      "|2017|February| 25|  18|     74|\n",
      "|2017| January|  8|   9|     75|\n",
      "|2017| January|  9|  18|     76|\n",
      "|2017| January| 19|   9|     77|\n",
      "|2017|   March|  9|   1|     78|\n",
      "|2017|   April| 24|  14|     79|\n",
      "|2017|    June| 25|   2|     80|\n",
      "|2017|     May| 16|   0|     81|\n",
      "|2017|     May| 21|  13|     82|\n",
      "|2017|    June| 18|  15|     83|\n",
      "|2017|     May| 30|  16|     84|\n",
      "|2017|     May|  8|   5|     85|\n",
      "|2017|     May| 24|   9|     86|\n",
      "|2017|     May| 26|   8|     87|\n",
      "|2017|     May| 29|   8|     88|\n",
      "|2017|    June|  3|   7|     89|\n",
      "|2017|    June|  6|  10|     90|\n",
      "|2017|    June| 28|  22|     91|\n",
      "|2017|   April|  8|  18|     92|\n",
      "|2017|     May| 22|   8|     93|\n",
      "|2017|    June| 21|  16|     94|\n",
      "|2017|   April| 19|   9|     95|\n",
      "|2017|   April| 29|   5|     96|\n",
      "|2017|    June| 11|  21|     97|\n",
      "|2017|   April|  8|   8|     98|\n",
      "|2017|     May| 27|  21|     99|\n",
      "|2017|   April| 25|  21|    100|\n",
      "+----+--------+---+----+-------+\n",
      "only showing top 100 rows"
     ]
    }
   ],
   "source": [
    "DIM_DATE.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe3112a2a5d4df8828b0f3d8f4fdd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|           card_type|card_id|\n",
      "+--------------------+-------+\n",
      "|Visa Dankort - on-us|      1|\n",
      "|  Mastercard - on-us|      2|\n",
      "|         HÃƒÂ¦vekort|      3|\n",
      "|            VisaPlus|      4|\n",
      "|     Dankort - on-us|      5|\n",
      "|        Visa Dankort|      6|\n",
      "| HÃƒÂ¦vekort - on-us|      7|\n",
      "|              CIRRUS|      8|\n",
      "|                VISA|      9|\n",
      "|             Maestro|     10|\n",
      "|          MasterCard|     11|\n",
      "|             Dankort|     12|\n",
      "+--------------------+-------+"
     ]
    }
   ],
   "source": [
    "DIM_CARD_TYPE.show(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2369373bda9d431c82939a7c1f3aee7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+--------------------+-------+-------+------+\n",
      "|atm_number|atm_manufacturer|        atm_location|      atm_streetname|atm_lat|atm_lon|atm_id|\n",
      "+----------+----------------+--------------------+--------------------+-------+-------+------+\n",
      "|        17|             NCR|             Randers|        ÃƒËœstervold| 56.462| 10.038|     1|\n",
      "|        23| Diebold Nixdorf|             Vodskov|          Vodskovvej| 57.104| 10.027|     2|\n",
      "|        73|             NCR|         HÃƒÂ¸jbjerg|      Rosenvangsalle| 56.119| 10.192|     3|\n",
      "|        86|             NCR|HillerÃƒÂ¸d IdrÃƒ...|          Milnersvej| 55.921| 12.299|     4|\n",
      "|        18| Diebold Nixdorf|              Viborg|           Toldboden| 56.448|  9.401|     5|\n",
      "|        67|             NCR|          HÃƒÂ¸jslev|      ÃƒËœsterrisvej| 56.551|   9.11|     6|\n",
      "|        84|             NCR|           Svendborg|  Sankt Nicolai Gade| 55.058| 10.609|     7|\n",
      "|        55|             NCR|               Arden|          Vestergade| 56.769|  9.859|     8|\n",
      "|        87|             NCR|Aalborg Storcente...|            Hobrovej| 57.005|  9.876|     9|\n",
      "|        81|             NCR|Spar KÃƒÂ¸bmand T...|       TornhÃƒÂ¸jvej| 57.026| 10.002|    10|\n",
      "|        28|             NCR|       LÃƒÂ¸gstÃƒÂ¸r|     ÃƒËœsterbrogade| 56.967|  9.253|    11|\n",
      "|        32|             NCR|          SÃƒÂ¦dding|         Tarphagevej| 55.498|  8.408|    12|\n",
      "|        24|             NCR|               Hobro|            Adelgade| 56.638|  9.794|    13|\n",
      "|        53|             NCR|         SÃƒÂ¦by Syd|Trafikcenter SÃƒÂ...| 57.313|  10.45|    14|\n",
      "|        22|             NCR|            Roskilde|    KÃƒÂ¸benhavnsvej| 55.642| 12.106|    15|\n",
      "|       102|             NCR|Aalborg Storcente...|            Hobrovej| 57.005|  9.876|    16|\n",
      "|        96|             NCR|      Aalborg Hallen|        Europa Plads| 57.044|  9.913|    17|\n",
      "|        35|             NCR|             Aabybro|        ÃƒËœstergade| 57.162|   9.73|    18|\n",
      "|        29|             NCR|      Skelagervej 15|         Skelagervej| 57.023|  9.891|    19|\n",
      "|       100|             NCR|       Intern  Skive|            Adelgade| 56.567|  9.027|    20|\n",
      "|        49|             NCR|            Bindslev|         NÃƒÂ¸rrebro| 57.541|   10.2|    21|\n",
      "|        36|             NCR|         HjÃƒÂ¸rring|        ÃƒËœstergade| 57.459|  9.988|    22|\n",
      "|        44|             NCR|              Lyngby|         Jernbanevej| 55.772|   12.5|    23|\n",
      "|        52|             NCR|            FarsÃƒÂ¸|              Torvet| 56.771|   9.34|    24|\n",
      "|        33|             NCR|               Vadum|      Ellehammersvej| 57.118|  9.861|    25|\n",
      "|        62| Diebold Nixdorf|            Terndrup|            Bymidten| 56.815| 10.057|    26|\n",
      "|         1|             NCR|          NÃƒÂ¦stved|         Farimagsvej| 55.233| 11.763|    27|\n",
      "|        82|             NCR|      Skallerup Klit|      Nordre Klitvej| 57.494|  9.838|    28|\n",
      "|        93|             NCR|Intern  BrÃƒÂ¸nde...|              Algade| 57.269|  9.945|    29|\n",
      "|        57|             NCR|         HillerÃƒÂ¸d|    KÃƒÂ¸benhavnsvej| 55.933| 12.314|    30|\n",
      "|        99|             NCR|       Intern  Vejle|         Havneparken| 55.707|  9.541|    31|\n",
      "|        56| Diebold Nixdorf|             KÃƒÂ¸ge|      SÃƒÂ¸ndre Alle| 55.454| 12.181|    32|\n",
      "|        65|             NCR|           Storvorde|     VandvÃƒÂ¦rksvej| 57.005| 10.101|    33|\n",
      "|        91|             NCR|         Skive Lobby|            Adelgade| 56.567|  9.027|    34|\n",
      "|         6|             NCR|          Fredericia|    SjÃƒÂ¦llandsgade| 55.564|  9.757|    35|\n",
      "|         8|             NCR|         GlyngÃƒÂ¸re|         FÃƒÂ¦rgevej| 56.762|  8.867|    36|\n",
      "|       106|             NCR|    Intern  Roskilde|    KÃƒÂ¸benhavnsvej| 55.642| 12.106|    37|\n",
      "|         9| Diebold Nixdorf|             Hadsund|           Storegade| 56.716| 10.114|    38|\n",
      "|        30|             NCR|    NykÃƒÂ¸bing Mors|         Kirketorvet| 56.795|   8.86|    39|\n",
      "|        47|             NCR|       Frederiksberg|     Gammel Kongevej| 55.677| 12.537|    40|\n",
      "|        72|             NCR|Daglig Brugsen Ãƒ...|           Kystvejen| 56.804| 10.271|    41|\n",
      "|        60|             NCR|           Hirtshals|JÃƒÂ¸rgen Fibiger...| 57.591|  9.957|    42|\n",
      "|        13|             NCR|             SÃƒÂ¦by|          Vestergade| 57.334| 10.515|    43|\n",
      "|       111| Diebold Nixdorf|              Aarhus|          Ceres Byen| 56.157| 10.194|    44|\n",
      "|        75|             NCR|           Nordkraft|     Kjellerups Torv| 57.047|  9.932|    45|\n",
      "|         7| Diebold Nixdorf|           Hjallerup|   Hjallerup Centret| 57.168| 10.148|    46|\n",
      "|        19|             NCR|             Kolding|            Vejlevej| 55.505|  9.457|    47|\n",
      "|       110| Diebold Nixdorf|           HolbÃƒÂ¦k|         Slotsvolden| 55.718| 11.704|    48|\n",
      "|        27|             NCR|             Herning|          Dalgasgade| 56.135|  8.971|    49|\n",
      "|        74|             NCR|             Jebjerg|           Kirkegade| 56.671|  9.013|    50|\n",
      "|        64|             NCR|             Horsens|     GrÃƒÂ¸nlandsvej| 55.859|  9.854|    51|\n",
      "|        51|             NCR|             Gistrup|          Hadsundvej| 56.997|  9.993|    52|\n",
      "|       112| Diebold Nixdorf|          NÃƒÂ¦stved|        Farimagsgade|  55.69| 12.575|    53|\n",
      "|        14|             NCR|          HÃƒÂ¸rning|        NÃƒÂ¸rrealle| 56.086| 10.037|    54|\n",
      "|        61|             NCR|             Esbjerg|        Strandbygade| 55.468|   8.44|    55|\n",
      "|        21|             NCR|ÃƒËœsterÃƒÂ¥  MÃƒ...|        ÃƒËœsterÃƒÂ¥| 57.049|  9.922|    56|\n",
      "|       107| Diebold Nixdorf|             Kolding|            Vejlevej| 55.505|  9.457|    57|\n",
      "|        97|             NCR|   Intern  Hjallerup|   Hjallerup Centret| 57.168| 10.148|    58|\n",
      "|        94|             NCR|       Intern Skagen|   Sct. Laurentiivej| 57.723|  10.59|    59|\n",
      "|        16|             NCR|               Skive|            Adelgade| 56.567|  9.027|    60|\n",
      "|        58|             NCR|         StÃƒÂ¸vring|         Baunebakken|  56.89|  9.836|    61|\n",
      "|        43|             NCR|                Aars|     Himmerlandsgade| 56.803|  9.518|    62|\n",
      "|        83|             NCR|                 Fur|          StenÃƒÂ¸re| 56.805|   9.02|    63|\n",
      "|         3|             NCR|               Ikast| RÃƒÂ¥dhusstrÃƒÂ¦det| 56.139|  9.154|    64|\n",
      "|        10|             NCR|      NÃƒÂ¸rresundby|              Torvet| 57.059|  9.922|    65|\n",
      "|        38|             NCR|            Hasseris|         Hasserisvej| 57.044|  9.898|    66|\n",
      "|        40| Diebold Nixdorf|       Frederikshavn|        Danmarksgade| 57.441| 10.537|    67|\n",
      "|        77|             NCR|     Aarhus Lufthavn|     Ny Lufthavnsvej| 56.308| 10.627|    68|\n",
      "|       105| Diebold Nixdorf|      NÃƒÂ¸rresundby|              Torvet| 57.059|  9.922|    69|\n",
      "|        98|             NCR|      Intern  Odense|        FÃƒÂ¦lledvej| 55.394|  10.37|    70|\n",
      "|        11|             NCR|           Sauersvej|Fridtjof Nansens Vej| 57.023|   9.94|    71|\n",
      "|       103| Diebold Nixdorf|            Vejgaard|          Hadsundvej| 57.043|   9.95|    72|\n",
      "|        50|             NCR|              Aarhus|      SÃƒÂ¸nder Alle| 56.153| 10.206|    73|\n",
      "|        59| Diebold Nixdorf|NykÃƒÂ¸bing Mors ...|         Kirketorvet| 56.795|   8.86|    74|\n",
      "|         5|             NCR|                Nibe|              Torvet| 56.983|  9.639|    75|\n",
      "|        71|             NCR|           AalbÃƒÂ¦k|          Centralvej| 57.593| 10.412|    76|\n",
      "|       113| Diebold Nixdorf|            Slagelse|  Mariendals AllÃƒÂ¨| 55.398| 11.342|    77|\n",
      "|        66|             NCR|         Brugsen ANS|  SÃƒÂ¸ndermarksgade| 56.306|  9.594|    78|\n",
      "|       101|             NCR|      Bryggen  Vejle|    SÃƒÂ¸nderbrogade| 55.705|  9.532|    79|\n",
      "|        15|             NCR|              Vestre|           Kastetvej| 57.053|  9.905|    80|\n",
      "|       109| Diebold Nixdorf|         Aalborg Syd|            Hobrovej| 57.005|  9.881|    81|\n",
      "|        41| Diebold Nixdorf|              Skagen|   Sct. Laurentiivej| 57.723|  10.59|    82|\n",
      "|        92|             NCR| Intern  HjÃƒÂ¸rring|        ÃƒËœstergade| 57.459|  9.988|    83|\n",
      "|        20|             NCR|         Bispensgade|         Bispensgade| 57.453|  9.996|    84|\n",
      "|       104|             NCR|Intern  ÃƒËœsterÃƒÂ¥|        ÃƒËœsterÃƒÂ¥| 57.049|  9.922|    85|\n",
      "|         4|             NCR|          Svogerslev|        BrÃƒÂ¸nsager| 55.634| 12.018|    86|\n",
      "|        90|             NCR|    Intern HolbÃƒÂ¦k|         Slotsvolden| 55.718| 11.704|    87|\n",
      "|        78| Diebold Nixdorf|              Nyborg|          Vestergade| 55.318| 10.781|    88|\n",
      "|        85| Diebold Nixdorf|        KÃƒÂ¸benhavn|      Regnbuepladsen| 55.676| 12.571|    89|\n",
      "|        69|             NCR|               Taars|            Bredgade| 57.385| 10.116|    90|\n",
      "|        48| Diebold Nixdorf|      BrÃƒÂ¸nderslev|              Algade| 57.269|  9.945|    91|\n",
      "|        68|             NCR|     Drive Up  Vejle|         Havneparken| 55.707|  9.541|    92|\n",
      "|        37|             NCR|           Silkeborg|          Borgergade| 56.179|  9.552|    93|\n",
      "|        63|             NCR|     Brugsen i Breum|       AakjÃƒÂ¦rsvej| 56.688|  9.069|    94|\n",
      "|        45|             NCR|          Abildgaard|      HjÃƒÂ¸rringvej| 57.447| 10.506|    95|\n",
      "|        12|             NCR|  ÃƒËœsterÃƒÂ¥  Duus|        ÃƒËœsterÃƒÂ¥| 57.049|  9.922|    96|\n",
      "|        25| Diebold Nixdorf|              Odense|        FÃƒÂ¦lledvej| 55.394|  10.37|    97|\n",
      "|        39|             NCR|           Svenstrup|      GodthÃƒÂ¥bsvej| 56.973|  9.851|    98|\n",
      "|        26|             NCR|           HolbÃƒÂ¦k|         Slotsvolden| 55.718| 11.704|    99|\n",
      "|        79|             NCR|          Middelfart|             Brogade| 55.507|  9.727|   100|\n",
      "|        80|             NCR|Menu KÃƒÂ¸bmand K...|           Klarupvej| 57.013| 10.046|   101|\n",
      "|        46| Diebold Nixdorf|        HelsingÃƒÂ¸r|      Sct. Olai Gade| 56.036| 12.612|   102|\n",
      "|        34|             NCR|           Skipperen|         Vestre Alle| 57.034|  9.908|   103|\n",
      "|        54|             NCR|               Durup|              Torvet| 56.745|  8.949|   104|\n",
      "|        31|             NCR|            Slagelse|     Mariendals Alle| 55.398| 11.342|   105|\n",
      "|       108|             NCR|   HÃƒÂ¸rning Hallen|            Toftevej| 56.091| 10.033|   106|\n",
      "|        70| Diebold Nixdorf|           Holstebro|         Hostrupsvej| 56.373|  8.625|   107|\n",
      "|        95|             NCR|Intern  KÃƒÂ¸benhavn|    RÃƒÂ¥dhuspladsen| 55.676| 12.571|   108|\n",
      "|        89|             NCR|Intern  Frederiks...|        Danmarksgade| 57.441| 10.537|   109|\n",
      "|        88|             NCR|  Storcenter indg. A|            Hobrovej| 57.005|  9.876|   110|\n",
      "|         2|             NCR|            Vejgaard|          Hadsundvej| 57.043|   9.95|   111|\n",
      "|        42|             NCR|            Vinderup|       SÃƒÂ¸ndergade| 56.481|  8.779|   112|\n",
      "|        76|             NCR|    DAYZ Feriecenter|          LivÃƒÂ¸vej| 56.893|  9.171|   113|\n",
      "+----------+----------------+--------------------+--------------------+-------+-------+------+"
     ]
    }
   ],
   "source": [
    "DIM_ATM.show(113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7dbbe7ec7c4e83b9211700c7737e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+-----------+-------+-------+-----------+\n",
      "|        atm_location|      atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|location_id|\n",
      "+--------------------+--------------------+-----------------+-----------+-------+-------+-----------+\n",
      "|             Kolding|            Vejlevej|              135|       6000| 55.505|  9.457|          1|\n",
      "|      Skelagervej 15|         Skelagervej|               15|       9000| 57.023|  9.891|          2|\n",
      "|    Intern HolbÃƒÂ¦k|         Slotsvolden|                7|       4300| 55.718| 11.704|          3|\n",
      "|              Odense|        FÃƒÂ¦lledvej|                3|       5000| 55.394|  10.37|          4|\n",
      "|               Ikast| RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|          5|\n",
      "|             Randers|        ÃƒËœstervold|               16|       8900| 56.462| 10.038|          6|\n",
      "|             Esbjerg|        Strandbygade|               20|       6700| 55.468|   8.44|          7|\n",
      "|              Lyngby|         Jernbanevej|                6|       2800| 55.772|   12.5|          8|\n",
      "|               Skive|            Adelgade|                8|       7800| 56.567|  9.027|          9|\n",
      "|               Durup|              Torvet|                4|       7870| 56.745|  8.949|         10|\n",
      "|             Hadsund|           Storegade|               12|       9560| 56.716| 10.114|         11|\n",
      "|          NÃƒÂ¦stved|         Farimagsvej|                8|       4700| 55.233| 11.763|         12|\n",
      "|               Hobro|            Adelgade|               31|       9500| 56.638|  9.794|         13|\n",
      "|           Nordkraft|     Kjellerups Torv|                1|       9000| 57.047|  9.932|         14|\n",
      "|            Slagelse|     Mariendals Alle|               29|       4200| 55.398| 11.342|         15|\n",
      "|                Aars|     Himmerlandsgade|               70|       9600| 56.803|  9.518|         16|\n",
      "|                 Fur|          StenÃƒÂ¸re|               19|       7884| 56.805|   9.02|         17|\n",
      "|            Hasseris|         Hasserisvej|              113|       9000| 57.044|  9.898|         18|\n",
      "|             SÃƒÂ¦by|          Vestergade|                3|       9300| 57.334| 10.515|         19|\n",
      "|        HelsingÃƒÂ¸r|      Sct. Olai Gade|               39|       3000| 56.036| 12.612|         20|\n",
      "|         Brugsen ANS|  SÃƒÂ¸ndermarksgade|               14|       8643| 56.306|  9.594|         21|\n",
      "|           Silkeborg|          Borgergade|               36|       8600| 56.179|  9.552|         22|\n",
      "|            Terndrup|            Bymidten|                2|       9575| 56.815| 10.057|         23|\n",
      "|           HolbÃƒÂ¦k|         Slotsvolden|                7|       4300| 55.718| 11.704|         24|\n",
      "|    DAYZ Feriecenter|          LivÃƒÂ¸vej|               80|       9681| 56.893|  9.171|         25|\n",
      "|           Svendborg|  Sankt Nicolai Gade|                1|       5700| 55.058| 10.609|         26|\n",
      "|    NykÃƒÂ¸bing Mors|         Kirketorvet|                1|       7900| 56.795|   8.86|         27|\n",
      "|            Slagelse|  Mariendals AllÃƒÂ¨|               29|       4200| 55.398| 11.342|         28|\n",
      "|         SÃƒÂ¦by Syd|Trafikcenter SÃƒÂ...|                1|       9300| 57.313|  10.45|         29|\n",
      "|             Gistrup|          Hadsundvej|              346|       9260| 56.997|  9.993|         30|\n",
      "|Menu KÃƒÂ¸bmand K...|           Klarupvej|               52|       9270| 57.013| 10.046|         31|\n",
      "|              Aarhus|      SÃƒÂ¸nder Alle|               11|       8000| 56.153| 10.206|         32|\n",
      "|           Sauersvej|Fridtjof Nansens Vej|                2|       9210| 57.023|   9.94|         33|\n",
      "|       LÃƒÂ¸gstÃƒÂ¸r|     ÃƒËœsterbrogade|                8|       9670| 56.967|  9.253|         34|\n",
      "|Aalborg Storcente...|            Hobrovej|              452|       9200| 57.005|  9.876|         35|\n",
      "|Aalborg Storcente...|            Hobrovej|              452|       9200| 57.005|  9.876|         36|\n",
      "|         Bispensgade|         Bispensgade|               35|       9800| 57.453|  9.996|         37|\n",
      "|            Vejgaard|          Hadsundvej|               20|       9000| 57.043|   9.95|         38|\n",
      "|        KÃƒÂ¸benhavn|      Regnbuepladsen|                5|       1550| 55.676| 12.571|         39|\n",
      "|Intern  KÃƒÂ¸benhavn|    RÃƒÂ¥dhuspladsen|               75|       1550| 55.676| 12.571|         40|\n",
      "|              Viborg|           Toldboden|                3|       8800| 56.448|  9.401|         41|\n",
      "|             Aabybro|        ÃƒËœstergade|                6|       9440| 57.162|   9.73|         42|\n",
      "|          Middelfart|             Brogade|                9|       5500| 55.507|  9.727|         43|\n",
      "|          HÃƒÂ¸jslev|      ÃƒËœsterrisvej|                2|       7840| 56.551|   9.11|         44|\n",
      "|               Arden|          Vestergade|                6|       9510| 56.769|  9.859|         45|\n",
      "|         HillerÃƒÂ¸d|    KÃƒÂ¸benhavnsvej|               31|       3400| 55.933| 12.314|         46|\n",
      "|            FarsÃƒÂ¸|              Torvet|                8|       9640| 56.771|   9.34|         47|\n",
      "|             KÃƒÂ¸ge|      SÃƒÂ¸ndre Alle|                1|       4600| 55.454| 12.181|         48|\n",
      "|       Frederiksberg|     Gammel Kongevej|              157|       1850| 55.677| 12.537|         49|\n",
      "|  Storcenter indg. A|            Hobrovej|              452|       9200| 57.005|  9.876|         50|\n",
      "|         HjÃƒÂ¸rring|        ÃƒËœstergade|                8|       9800| 57.459|  9.988|         51|\n",
      "|          NÃƒÂ¦stved|        Farimagsgade|                8|       4700|  55.69| 12.575|         52|\n",
      "|          Svogerslev|        BrÃƒÂ¸nsager|                1|       4000| 55.634| 12.018|         53|\n",
      "|      Skallerup Klit|      Nordre Klitvej|               21|       9800| 57.494|  9.838|         54|\n",
      "|     Drive Up  Vejle|         Havneparken|                4|       7100| 55.707|  9.541|         55|\n",
      "|ÃƒËœsterÃƒÂ¥  MÃƒ...|        ÃƒËœsterÃƒÂ¥|               12|       9000| 57.049|  9.922|         56|\n",
      "|           Hjallerup|   Hjallerup Centret|               18|       9320| 57.168| 10.148|         57|\n",
      "|NykÃƒÂ¸bing Mors ...|         Kirketorvet|                1|       7900| 56.795|   8.86|         58|\n",
      "|Intern  Frederiks...|        Danmarksgade|               48|       9900| 57.441| 10.537|         59|\n",
      "|      Intern  Odense|        FÃƒÂ¦lledvej|                3|       5000| 55.394|  10.37|         60|\n",
      "|          HÃƒÂ¸rning|        NÃƒÂ¸rrealle|               12|       8362| 56.086| 10.037|         61|\n",
      "| Intern  HjÃƒÂ¸rring|        ÃƒËœstergade|                8|       9800| 57.459|  9.988|         62|\n",
      "|               Vadum|      Ellehammersvej|               43|       9430| 57.118|  9.861|         63|\n",
      "|          Fredericia|    SjÃƒÂ¦llandsgade|               33|       7000| 55.564|  9.757|         64|\n",
      "|     Aarhus Lufthavn|     Ny Lufthavnsvej|               24|       8560| 56.308| 10.627|         65|\n",
      "|             Vodskov|          Vodskovvej|               27|       9310| 57.104| 10.027|         66|\n",
      "|         Skive Lobby|            Adelgade|                8|       7800| 56.567|  9.027|         67|\n",
      "|             Jebjerg|           Kirkegade|                4|       7870| 56.671|  9.013|         68|\n",
      "|          Abildgaard|      HjÃƒÂ¸rringvej|              144|       9900| 57.447| 10.506|         69|\n",
      "|  ÃƒËœsterÃƒÂ¥  Duus|        ÃƒËœsterÃƒÂ¥|               12|       9000| 57.049|  9.922|         70|\n",
      "|           Hirtshals|JÃƒÂ¸rgen Fibiger...|                4|       9850| 57.591|  9.957|         71|\n",
      "|      Bryggen  Vejle|    SÃƒÂ¸nderbrogade|                2|       7100| 55.705|  9.532|         72|\n",
      "|       Intern  Skive|            Adelgade|                8|       7800| 56.567|  9.027|         73|\n",
      "|              Skagen|   Sct. Laurentiivej|               36|       9990| 57.723|  10.59|         74|\n",
      "|           Skipperen|         Vestre Alle|                2|       9000| 57.034|  9.908|         75|\n",
      "|   Intern  Hjallerup|   Hjallerup Centret|               18|       9320| 57.168| 10.148|         76|\n",
      "|       Intern  Vejle|         Havneparken|                4|       7100| 55.707|  9.541|         77|\n",
      "|            Roskilde|    KÃƒÂ¸benhavnsvej|               65|       4000| 55.642| 12.106|         78|\n",
      "|      BrÃƒÂ¸nderslev|              Algade|                4|       9700| 57.269|  9.945|         79|\n",
      "|           Holstebro|         Hostrupsvej|                6|       7500| 56.373|  8.625|         80|\n",
      "|              Nyborg|          Vestergade|               35|       5800| 55.318| 10.781|         81|\n",
      "|Spar KÃƒÂ¸bmand T...|       TornhÃƒÂ¸jvej|                4|       9220| 57.026| 10.002|         82|\n",
      "|Intern  ÃƒËœsterÃƒÂ¥|        ÃƒËœsterÃƒÂ¥|               12|       9000| 57.049|  9.922|         83|\n",
      "|     Brugsen i Breum|       AakjÃƒÂ¦rsvej|                1|       7870| 56.688|  9.069|         84|\n",
      "|              Aarhus|          Ceres Byen|               75|       8000| 56.157| 10.194|         85|\n",
      "|         HÃƒÂ¸jbjerg|      Rosenvangsalle|              194|       8270| 56.119| 10.192|         86|\n",
      "|       Frederikshavn|        Danmarksgade|               48|       9900| 57.441| 10.537|         87|\n",
      "|      Aalborg Hallen|        Europa Plads|                4|       9000| 57.044|  9.913|         88|\n",
      "|           Storvorde|     VandvÃƒÂ¦rksvej|                2|       9280| 57.005| 10.101|         89|\n",
      "|    Intern  Roskilde|    KÃƒÂ¸benhavnsvej|               65|       4000| 55.642| 12.106|         90|\n",
      "|          SÃƒÂ¦dding|         Tarphagevej|               59|       6710| 55.498|  8.408|         91|\n",
      "|               Taars|            Bredgade|               91|       9830| 57.385| 10.116|         92|\n",
      "|   HÃƒÂ¸rning Hallen|            Toftevej|               53|       8362| 56.091| 10.033|         93|\n",
      "|         GlyngÃƒÂ¸re|         FÃƒÂ¦rgevej|                1|       7870| 56.762|  8.867|         94|\n",
      "|           Svenstrup|      GodthÃƒÂ¥bsvej|               14|       9230| 56.973|  9.851|         95|\n",
      "|      NÃƒÂ¸rresundby|              Torvet|                6|       9400| 57.059|  9.922|         96|\n",
      "|            Vinderup|       SÃƒÂ¸ndergade|                5|       7830| 56.481|  8.779|         97|\n",
      "|              Vestre|           Kastetvej|               36|       9000| 57.053|  9.905|         98|\n",
      "|                Nibe|              Torvet|                1|       9240| 56.983|  9.639|         99|\n",
      "|             Herning|          Dalgasgade|               30|       7400| 56.135|  8.971|        100|\n",
      "|           AalbÃƒÂ¦k|          Centralvej|                5|       9982| 57.593| 10.412|        101|\n",
      "|         Aalborg Syd|            Hobrovej|              440|       9200| 57.005|  9.881|        102|\n",
      "|             Horsens|     GrÃƒÂ¸nlandsvej|                5|       8700| 55.859|  9.854|        103|\n",
      "|            Bindslev|         NÃƒÂ¸rrebro|               18|       9881| 57.541|   10.2|        104|\n",
      "|HillerÃƒÂ¸d IdrÃƒ...|          Milnersvej|               39|       3400| 55.921| 12.299|        105|\n",
      "|Daglig Brugsen Ãƒ...|           Kystvejen|               51|       9560| 56.804| 10.271|        106|\n",
      "|         StÃƒÂ¸vring|         Baunebakken|                4|       9530|  56.89|  9.836|        107|\n",
      "|Intern  BrÃƒÂ¸nde...|              Algade|                4|       9700| 57.269|  9.945|        108|\n",
      "|       Intern Skagen|   Sct. Laurentiivej|               36|       9990| 57.723|  10.59|        109|\n",
      "+--------------------+--------------------+-----------------+-----------+-------+-------+-----------+"
     ]
    }
   ],
   "source": [
    "DIM_LOCATION.show(109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc76626e195a4fa3bd6bdb0b3877e7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+----+------------------+----------+----------------+--------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "|year|  month|day|hour|         card_type|atm_number|atm_manufacturer|  atm_location|     atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|atm_status|currency|   service|transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main| weather_description|trans_id|\n",
      "+----+-------+---+----+------------------+----------+----------------+--------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "|2017|January|  1|   0|        MasterCard|         2|             NCR|      Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|  Inactive|     DKK|Withdrawal|              1764|        null|        null|   0.59|        92|       500|        Rain|          light rain|       1|\n",
      "|2017|January|  1|   0|              VISA|         2|             NCR|      Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|  Inactive|     DKK|Withdrawal|              1891|        null|        null|   0.59|        92|       500|        Rain|          light rain|       2|\n",
      "|2017|January|  1|   0|              VISA|         3|             NCR|         Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|  Inactive|     DKK|Withdrawal|              4166|        null|        null|    0.0|        75|       300|     Drizzle|light intensity d...|       3|\n",
      "|2017|January|  1|   0|        MasterCard|         4|             NCR|    Svogerslev|       BrÃƒÂ¸nsager|                1|       4000| 55.634| 12.018|    Active|     DKK|Withdrawal|              5153|        null|        null|    0.0|        88|       701|        Mist|                mist|       4|\n",
      "|2017|January|  1|   0|        MasterCard|         5|             NCR|          Nibe|             Torvet|                1|       9240| 56.983|  9.639|    Active|     DKK|Withdrawal|              3269|        null|        null|   0.59|        92|       500|        Rain|          light rain|       5|\n",
      "|2017|January|  1|   0|        MasterCard|         6|             NCR|    Fredericia|   SjÃƒÂ¦llandsgade|               33|       7000| 55.564|  9.757|    Active|     DKK|Withdrawal|               887|        null|        null|   0.29|        92|       500|        Rain|          light rain|       6|\n",
      "|2017|January|  1|   0|Mastercard - on-us|         7| Diebold Nixdorf|     Hjallerup|  Hjallerup Centret|               18|       9320| 57.168| 10.148|    Active|     DKK|Withdrawal|              4626|        null|        null|   0.59|        92|       500|        Rain|          light rain|       7|\n",
      "|2017|January|  1|   0|        MasterCard|         8|             NCR|   GlyngÃƒÂ¸re|        FÃƒÂ¦rgevej|                1|       7870| 56.762|  8.867|    Active|     DKK|Withdrawal|               470|        null|        null|    0.0|        75|       300|     Drizzle|light intensity d...|       8|\n",
      "|2017|January|  1|   0|              VISA|         9| Diebold Nixdorf|       Hadsund|          Storegade|               12|       9560| 56.716| 10.114|    Active|     DKK|Withdrawal|              8473|        null|        null|   0.59|        92|       500|        Rain|          light rain|       9|\n",
      "|2017|January|  1|   0|           Dankort|        10|             NCR|NÃƒÂ¸rresundby|             Torvet|                6|       9400| 57.059|  9.922|    Active|     DKK|Withdrawal|               953|        null|        null|   0.59|        92|       500|        Rain|          light rain|      10|\n",
      "+----+-------+---+----+------------------+----------+----------------+--------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "FACT_ATM_TRANS.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d92467aaa1448394884cfeeec92a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Create TimeStamp in Date Dimention Table from year,month,date and hour\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as func\n",
    "### Convert month into numeric format\n",
    "DIM_DATE=DIM_DATE.withColumn(\"month\",from_unixtime(unix_timestamp(col(\"month\"),'MMM'),'MM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75909d18b264159a2a0627d85c54b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-------+\n",
      "|year|month|day|hour|date_id|\n",
      "+----+-----+---+----+-------+\n",
      "|2017|   01| 23|   2|      1|\n",
      "|2017|   02| 26|   8|      2|\n",
      "|2017|   03| 12|  13|      3|\n",
      "|2017|   03| 14|  21|      4|\n",
      "|2017|   03| 21|   7|      5|\n",
      "+----+-----+---+----+-------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "DIM_DATE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bd0cc01ad54ffab99febf433618efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##create date field from year,month,day\n",
    "cols=[\"year\",\"month\",\"day\"]\n",
    "DIM_DATE=DIM_DATE.withColumn(\"date\",concat_ws(\"-\",*cols).cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b313720133634974b87684e39900b453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-------+----------+\n",
      "|year|month|day|hour|date_id|      date|\n",
      "+----+-----+---+----+-------+----------+\n",
      "|2017|   01| 23|   2|      1|2017-01-23|\n",
      "|2017|   02| 26|   8|      2|2017-02-26|\n",
      "|2017|   03| 12|  13|      3|2017-03-12|\n",
      "|2017|   03| 14|  21|      4|2017-03-14|\n",
      "|2017|   03| 21|   7|      5|2017-03-21|\n",
      "+----+-----+---+----+-------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "DIM_DATE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c44bd93508348d384b2dc1f8214cec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Create full_date_time from date and hour \n",
    "DIM_DATE=DIM_DATE.withColumn(\"full_date_time\",func.date_format(func.to_timestamp(func.concat(\"date\",\"hour\"),\"yyyy-MM-ddHH\"),\"yyyy-MM-dd hh:ss:SSa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7b67c2f77d4e89a37712620f62a719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-------+----------+--------------------+\n",
      "|year|month|day|hour|date_id|      date|      full_date_time|\n",
      "+----+-----+---+----+-------+----------+--------------------+\n",
      "|2017|   01| 23|   2|      1|2017-01-23|2017-01-23 02:00:...|\n",
      "|2017|   02| 26|   8|      2|2017-02-26|2017-02-26 08:00:...|\n",
      "|2017|   03| 12|  13|      3|2017-03-12|2017-03-12 01:00:...|\n",
      "+----+-----+---+----+-------+----------+--------------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "DIM_DATE.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c611632af34776a3f8348f5f8fdd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###revert the month from numeric back to string\n",
    "DIM_DATE=DIM_DATE.withColumn(\"month\",from_unixtime(unix_timestamp(col(\"month\"),'MM'),'MMM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e23be7cdd444e0aea33a875ea1904b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-------+----------+--------------------+\n",
      "|year|month|day|hour|date_id|      date|      full_date_time|\n",
      "+----+-----+---+----+-------+----------+--------------------+\n",
      "|2017|  Jan| 23|   2|      1|2017-01-23|2017-01-23 02:00:...|\n",
      "|2017|  Feb| 26|   8|      2|2017-02-26|2017-02-26 08:00:...|\n",
      "|2017|  Mar| 12|  13|      3|2017-03-12|2017-03-12 01:00:...|\n",
      "+----+-----+---+----+-------+----------+--------------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "DIM_DATE.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5539dfccda44f84816076182b3d6baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##cast as timestamptype\n",
    "DIM_DATE=DIM_DATE.withColumn('full_date_time',func.unix_timestamp('full_date_time','yyyy-MM-dd HH:mm:ss').cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9d81862a6c4ddd974db8407dfa7a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+--------+-------+----------+-------------------+\n",
      "|year|month|day|hour| weekday|date_id|      date|     full_date_time|\n",
      "+----+-----+---+----+--------+-------+----------+-------------------+\n",
      "|2017|  Jan|  5|  21|Thursday|      1|2017-01-05|2017-01-05 09:00:00|\n",
      "|2017|  Jan| 22|  15|  Sunday|      2|2017-01-22|2017-01-22 03:00:00|\n",
      "|2017|  Apr|  7|   9|  Friday|      3|2017-04-07|2017-04-07 09:00:00|\n",
      "+----+-----+---+----+--------+-------+----------+-------------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "DIM_DATE.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b4b81c5b02452388fafbcf530d1e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+----------+----------+----------------+------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "|year|month|day|hour| card_type|atm_number|atm_manufacturer|atm_location|     atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|atm_status|currency|   service|transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main| weather_description|trans_id|\n",
      "+----+-----+---+----+----------+----------+----------------+------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "|2017|  Jan|  1|   0|MasterCard|         2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|  Inactive|     DKK|Withdrawal|              1764|        null|        null|   0.59|        92|       500|        Rain|          light rain|       1|\n",
      "|2017|  Jan|  1|   0|      VISA|         2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|  Inactive|     DKK|Withdrawal|              1891|        null|        null|   0.59|        92|       500|        Rain|          light rain|       2|\n",
      "|2017|  Jan|  1|   0|      VISA|         3|             NCR|       Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|  Inactive|     DKK|Withdrawal|              4166|        null|        null|    0.0|        75|       300|     Drizzle|light intensity d...|       3|\n",
      "+----+-----+---+----+----------+----------+----------------+------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "### Perform the same transformation on FACT_ATM_TRANS as in DIM_DATE for the upcoming join function\n",
    "### Needed because in DIM_DATE the transformation is Jaunary ---> 01 ---> Jan\n",
    "FACT_ATM_TRANS=FACT_ATM_TRANS.withColumn(\"month\",from_unixtime(unix_timestamp(col(\"month\"),'MMM'),'MM'))\n",
    "FACT_ATM_TRANS=FACT_ATM_TRANS.withColumn(\"month\",from_unixtime(unix_timestamp(col(\"month\"),'MM'),'MMM'))\n",
    "FACT_ATM_TRANS.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347ced2ad08440579cb7a7b0b12677b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### REARRANGING COLUMNS [ Dimensions Table]\n",
    "DIM_DATE = DIM_DATE.select(\"date_id\",\"full_date_time\",\"year\",\"month\",\"day\",\"hour\",\"weekday\")\n",
    "DIM_CARD_TYPE = DIM_CARD_TYPE.select(\"card_id\",\"card_type\")\n",
    "DIM_ATM = DIM_ATM.select(\"atm_id\",\"atm_number\",\"atm_manufacturer\",\"atm_location\",\"atm_streetname\",\"atm_lat\",\"atm_lon\")\n",
    "DIM_LOCATION = DIM_LOCATION.select(\"location_id\",\"atm_location\",\"atm_streetname\",\"atm_street_number\",\"atm_zipcode\",\"atm_lat\",\"atm_lon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e831232d900b44e9a032e0494868372a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+----+----------+----------+----------------+------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+-------+--------------+-------+\n",
      "|year|  month|day|hour| card_type|atm_number|atm_manufacturer|atm_location|     atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|atm_status|currency|   service|transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main| weather_description|trans_id|date_id|full_date_time|weekday|\n",
      "+----+-------+---+----+----------+----------+----------------+------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+-------+--------------+-------+\n",
      "|2017|January|  1|   0|MasterCard|         2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|  Inactive|     DKK|Withdrawal|              1764|        null|        null|   0.59|        92|       500|        Rain|          light rain|       1|   null|          null|   null|\n",
      "|2017|January|  1|   0|      VISA|         2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|  Inactive|     DKK|Withdrawal|              1891|        null|        null|   0.59|        92|       500|        Rain|          light rain|       2|   null|          null|   null|\n",
      "|2017|January|  1|   0|      VISA|         3|             NCR|       Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|  Inactive|     DKK|Withdrawal|              4166|        null|        null|    0.0|        75|       300|     Drizzle|light intensity d...|       3|   null|          null|   null|\n",
      "+----+-------+---+----+----------+----------+----------------+------------+-------------------+-----------------+-----------+-------+-------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+-------+--------------+-------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "### Perform join to import *_id's to FACT_ATM_TRANS\n",
    "FACT_ATM_TRANS=FACT_ATM_TRANS.join(DIM_DATE,on=['year','month','day','hour'],how=\"left\")\n",
    "FACT_ATM_TRANS.show(3)\n",
    "FACT_ATM_TRANS=FACT_ATM_TRANS.join(DIM_CARD_TYPE,on=['card_type'],how=\"left\")\n",
    "FACT_ATM_TRANS=FACT_ATM_TRANS.join(DIM_ATM,on=['atm_number','atm_manufacturer'],how=\"left\")\n",
    "FACT_ATM_TRANS=FACT_ATM_TRANS.join(DIM_LOCATION,on=['atm_location','atm_streetname','atm_lat','atm_lon'],how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef069d28cdf14a1a89c6abdc40d7c708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FACT_ATM_TRANS.drop(\"year\",\"month\",\"day\",\"hour\",\"card_type\",\"atm_number\",\"atm_manufacturer\",\"atm_location\",\"atm_streetname\",\"atm_street_number\",\"atm_zipcode\",\"atm_lat\",\"atm_lon\")\n",
    "###Rearranging Fact table and rename columns per target schema\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.select(\"trans_id\",\"atm_id\",\"location_id\",\"date_id\",\"card_id\",\"atm_status\",\"currency\",\"service\",\"transaction_amount\",\"message_code\",\"message_text\",\"rain_3h\",\"clouds_all\",\"weather_id\",\"weather_main\",\"weather_description\")\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.withColumnRenamed(\"location_id\",\"weather_loc_id\")\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.withColumnRenamed(\"card_id\",\"card_type_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600576bbdd484bd5a96d9a8a6810701e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DIM_CARD_TYPE = DIM_CARD_TYPE.withColumnRenamed(\"card_id\",\"card_type_id\")\n",
    "DIM_LOCATION = DIM_LOCATION.withColumnRenamed(\"atm_street_number\",\"street_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcb79d4073d4b719ac0aeec963ac699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Perform join to import location_id to DIM_ATM\n",
    "DIM_ATM=DIM_ATM.join(DIM_LOCATION,on=['atm_location','atm_streetname','atm_lat','atm_lon'],how=\"left\")\n",
    "DIM_ATM=DIM_ATM.select(\"atm_id\",\"atm_number\",\"atm_manufacturer\",\"location_id\")\n",
    "DIM_ATM=DIM_ATM.withColumnRenamed(\"location_id\",\"atm_location_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82e8d3fc88b4d23a95f220aea9364b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Rename according to target schema\n",
    "DIM_LOCATION=DIM_LOCATION.withColumnRenamed(\"atm_location\",\"location\")\n",
    "DIM_LOCATION=DIM_LOCATION.withColumnRenamed(\"atm_streetname\",\"streetname\")\n",
    "DIM_LOCATION=DIM_LOCATION.withColumnRenamed(\"atm_zipcode\",\"zipcode\")\n",
    "DIM_LOCATION=DIM_LOCATION.withColumnRenamed(\"atm_lat\",\"lat\")\n",
    "DIM_LOCATION=DIM_LOCATION.withColumnRenamed(\"atm_lon\",\"lon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28dc5ef9e394bf390ec45cb5b8b553b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- full_date_time: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- atm_number: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location_id: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- streetname: string (nullable = true)\n",
      " |-- street_number: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- weather_loc_id: integer (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- rain_3h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "### Verify the columns - per target schema\n",
    "DIM_DATE.printSchema()\n",
    "DIM_ATM.printSchema()\n",
    "DIM_LOCATION.printSchema()\n",
    "DIM_CARD_TYPE.printSchema()\n",
    "FACT_ATM_TRANS.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d42489f937746c884fb9a6283c1dfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8685"
     ]
    }
   ],
   "source": [
    "### validating final counts \n",
    "DIM_DATE.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b1ba34095f4fb281047e62be78b817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113"
     ]
    }
   ],
   "source": [
    "DIM_ATM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c605df5f8d4daa8bca40a87e46a801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109"
     ]
    }
   ],
   "source": [
    "DIM_LOCATION.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10642901be240d9811b60ec1b5502d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12"
     ]
    }
   ],
   "source": [
    "DIM_CARD_TYPE.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b3d12f5a9e4345ab6c5d3567bd054a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2468571"
     ]
    }
   ],
   "source": [
    "FACT_ATM_TRANS.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3645e531fe2f4bdaa6c089c122b47d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o1114.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 932, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1114.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##writing to s3. Bucket should be created in advance of running this function\n",
    "DIM_CARD_TYPE.write.csv('s3a://upgrad-etlproject-vinay/dim_card_type/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0534ec1e016a4535b0bd6732758e8cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o1155.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 932, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1155.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DIM_LOCATION.write.csv('s3a://upgrad-etlproject-vinay/dim_location/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e61e7537984b15a61ebd589a280b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o1196.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 932, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1196.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DIM_ATM.write.csv('s3a://upgrad-etlproject-vinay/dim_atm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2077084db274862b049918dda1fd0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o1237.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 932, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1237.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DIM_DATE.write.csv('s3a://upgrad-etlproject-vinay/dim_date/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358cbd67b2134da78d9a729b1da19faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o1278.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 932, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1278.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:165)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories.\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:399)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:460)\n",
      "\tat org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:199)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:67)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:591)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:799)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:422)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:360)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter.commitJob(DirectFileOutputCommitter.java:111)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.commitJob(SQLEmrOptimizedCommitProtocol.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:187)\n",
      "\t... 37 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FACT_ATM_TRANS.write.csv('s3a://upgrad-etlproject-vinay/fact_atm_trans/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
